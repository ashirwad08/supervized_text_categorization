{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: This is a rough workpad. Steps may not be sequential and there is no coherent narrative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import h5py\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'dataset_1'])\n",
      "('Shape of the array dataset_1: \\n', (14048, 50))\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./../pickles/final_training_data.h5','r') as hf:\n",
    "    print('List of arrays in this file: \\n', hf.keys())\n",
    "    Xtrain = np.array(hf.get('dataset_1'))\n",
    "    print('Shape of the array dataset_1: \\n', Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'dataset_1'])\n",
      "('Shape of the array dataset_1: \\n', (3599, 50))\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('./../pickles/final_test_data.h5','r') as hf:\n",
    "    print('List of arrays in this file: \\n', hf.keys())\n",
    "    Xtest = np.array(hf.get('dataset_1'))\n",
    "    print('Shape of the array dataset_1: \\n', Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14048, 50), (3599, 50))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = pd.read_csv('./../pickles/training_targets.csv')\n",
    "ytest = pd.read_csv('./../pickles/test_targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14048, 1), (3599, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #encode labels if needed\n",
    "\n",
    "# outcomes = targets.category.unique()\n",
    "# i = range(len(outcomes))\n",
    "\n",
    "# label_bins_tups = zip(outcomes, i)\n",
    "\n",
    "# label_bins = OrderedDict(label_bins_tups)\n",
    "\n",
    "# label_bins.values()\n",
    "\n",
    "# targets['ybinarized'] = targets.category.map(label_bins)\n",
    "\n",
    "# ybin = targets.ybinarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an exploratory run back in the \"Build_Test_Train\" notebook, I discovered that Logistic Regression, and Random Forests provide (with default parameters) and with **class weights based sampling** turned on, gave the highest and second highest Macro F1 score on the hold out sets, *respectively*. I'll now run a Grid Search to tune the parameters for each algorithm and the discerning score will be the macro F1 score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def best_config(model_info, parameters, train_instances, judgements, cv):\n",
    "    \"\"\"\n",
    "    Takes in a classifier model with a grid of parameter values to combinatorially explore.\n",
    "    Returns classifier object with best configuration, the best tuning score (macro f1) \n",
    "    \"\"\"\n",
    "    [name, model] = model_info\n",
    "    print 'Grid search for... ' + name\n",
    "    clf = GridSearchCV(model, parameters, cv=cv, scoring=\"f1_macro\", verbose=1, n_jobs=-1)\n",
    "    clf.fit(train_instances, np.array(judgements).ravel())\n",
    "    best_estimator = clf.best_estimator_\n",
    "    print 'Best configuration: ' + str(clf.best_params_) + 'Best CV score (macro f1): ' + str(clf.best_score_)\n",
    "    return [str(clf.best_params_), clf.best_score_, best_estimator, clf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the best model from a set of model families given  training data using crosvalidation\n",
    "def best_model(classifier_families, train_instances, judgements, cv, holdout_feats, holdout_targets):\n",
    "    \"\"\"\n",
    "    sends each model and parameter grid specifid in classifier_families to the best_config function to \n",
    "    tune and cross validate over the parameters. \n",
    "    Returns the best performing classifier amongst them all\n",
    "    \"\"\"\n",
    "    best_quality = 0.0\n",
    "    best_classifier = None    \n",
    "    classifiers = []\n",
    "     #Update ash:\n",
    "    #Because I'm forcing the training with balanced samples (with the possibility of over sampling)\n",
    "    #I should retest each classifier on the holdout test set. The scores of this one will be used to gauage the \n",
    "    #best classifier\n",
    "    \n",
    "    #(fig, (ax1, ax2, ax3)) = plt.subplots(ncols=1, nrows=3, figsize=(15,10))\n",
    "#     plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random Chance')\n",
    "#     plt.set_xlim([-0.05, 1.05])\n",
    "#     plt.set_ylim([-0.05, 1.05])\n",
    "#     plt.set_xlabel('False Positive Rate')\n",
    "#     plt.set_ylabel('True Positive Rate')\n",
    "#     plt.set_title('Receiver Operating Characteristic)')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.tight_layout()\n",
    "        \n",
    "    for name, model, parameters in classifier_families:\n",
    "        \n",
    "        bestparams, bestscore, bestest, clf = best_config([name, model], parameters, train_instances, judgements, cv)\n",
    "        holdout_preds = clf.predict(holdout_feats) #predict using best model on holdout set\n",
    "        #probas_ = clf.predict_proba(holdout_feats)\n",
    "        \n",
    "        mf1 =  f1_score(holdout_targets, holdout_preds,average='macro')\n",
    "        \n",
    "#         #plot ROC \n",
    "#         fpr, tpr, thresholds = roc_curve(holdout_targets, probas_[:, 1])\n",
    "#         auc = roc_auc_score(holdout_targets, probas_[:, 1],average='macro')\n",
    "#         plt.plot(fpr, tpr, lw=1, label='%s has macro f1 = %f and avg. auc = %f' % (name, mf1, auc))\n",
    "     \n",
    "        print 'Holdout Macro F1 performance for %s = %f \\n' % (name, mf1)\n",
    "\n",
    "        classifiers.append((bestparams, bestscore, bestest, mf1))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    for name, cv_quality, classifier, holdout_quality in classifiers:\n",
    "        print 'Considering classifier... ' + name\n",
    "        if (holdout_quality > best_quality):\n",
    "            best_quality = holdout_quality\n",
    "            best_classifier = [name, classifier]\n",
    "\n",
    "    print 'Best classifier... ' + best_classifier[0]\n",
    "    return best_classifier[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def candidate_families():\n",
    "    \"\"\"\n",
    "    Setup to compare classifiers by specifying their tuning parameters to be tested and \n",
    "    by a grid search using the Macro F1 score to pick the winner\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    #SVM Linear Kernel (probably better for high dim space)\n",
    "    svm_tuned_parameters = [{'kernel': ['linear','rbf'], \n",
    "                            'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                             'gamma':[0.0000001, 0.000001, 0.0001, 0.001, 0.1, 1, 10, 100],\n",
    "                            'class_weight':['balanced',None]}]\n",
    "    candidates.append([\"SVM\", SVC(), svm_tuned_parameters])\n",
    "    \n",
    "    Random Forest\n",
    "    rf_tuned_parameters = [{\"n_estimators\": [75, 500, 1500],\n",
    "                           \"criterion\":['gini','entropy'],\n",
    "                           \"class_weight\":['balanced','balanced_subsample'],\n",
    "                           \"min_samples_leaf\":[5, 15, 50, 150]}]\n",
    "    candidates.append([\"RandomForest\", RandomForestClassifier(n_jobs=6), rf_tuned_parameters]) \n",
    "    \n",
    "    #Logistic Regression\n",
    "    LR_tuned_parameters = [{\"penalty\": ['l1','l2'],\n",
    "                           \"class_weight\":[None,'balanced'],\n",
    "                           \"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "    candidates.append([\"LogisticRegression\", LogisticRegression(), LR_tuned_parameters])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = best_model(candidate_families(), Xtrain, ytrain, 5, Xtest, ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the error, we discovered that the SVM with the parameters below still performed horribly on the Holdout set with a Macro F1 of 0.068256.\n",
    "\n",
    "> Best configuration: {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1, 'class_weight': 'balanced'}   \n",
    "Cross Validation F1 Score (balanced dataset) = 0.216158415487  \n",
    "\n",
    "> Holdout Macro F1 performance for SVM = 0.068256 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = best_model(candidate_families(), Xtrain, ytrain, 3, Xtest, ytest) \n",
    "#re-running after some code tweaks and commenting out SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for... LogisticRegression\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n"
     ]
    }
   ],
   "source": [
    "classifier = best_model(candidate_families(), Xtrain, ytrain, 3, Xtest, ytest) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**have to redo. Forgot to transform test set using training vectorizer!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_ROC_PR(Xtrain, ytrain, k, rf_estimators, class_subsampling=None, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve and AUC metric for given dataset, along with k-fold cross validation.\n",
    "    Use a Random Forest classifier with ideal number of estimators pre-determined\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(np.array(ytrain).ravel(), n_folds=k) #preserves class %\n",
    "    clf_rf = RandomForestClassifier(n_estimators=rf_estimators, verbose=0, criterion='gini', \n",
    "                                    n_jobs=-1, class_weight=class_subsampling) \n",
    "    #clf_LR = LogisticRegression(n_jobs=-1, class_weight=class_subsampling)\n",
    "    \n",
    "    #plt.figure(figsize=(15,7))\n",
    "    #(fig, (ax1, ax2)) = plt.subplots(ncols=1, nrows=2, figsize=(15,10))\n",
    "    for i, (train, cval) in enumerate(cv):\n",
    "        print 'fitting LR on cv run {}...\\n'.format(i)\n",
    "        clf_rf.fit(Xtrain[train], np.array(ytrain[train]).ravel())\n",
    "        #clf_LR.fit(Xtrain[train], np.array(ytrain[train]).ravel())\n",
    "\n",
    "        probas_ = clf_rf.predict_proba(Xtrain[cval])\n",
    "        #probas_ = clf_LR.predict_proba(Xtrain[cval])\n",
    "\n",
    "        ypred = (probas_[:,1] > cutoff).astype(int)\n",
    "        \n",
    "        # Compute ROC curve and area under the curve\n",
    "        #fpr, tpr, thresholds = roc_curve(ytrain.iloc[cval], probas_[:, 1], pos_label=1)\n",
    "        #precision, recall, thresholds = precision_recall_curve(ytrain.iloc[cval], probas_[:,1], pos_label=1) #sample_weight=np.where(ytrain.iloc[train]==1, 1.0, 30.0).ravel())\n",
    "        f1 = f1_score(ytrain[cval], ypred, labels=label_bins.values(), average='macro')\n",
    "        print 'Cross Validation #{} macro F1 score = {}\\n'.format(i, f1)\n",
    "        \n",
    "        print 'Classification Report for CV run {}: \\n'.format(i)\n",
    "        print(classification_report(ytrain[cval], ypred, labels=label_bins.values(), \n",
    "                                    target_names=label_bins.keys(), \n",
    "                                    sample_weight=None, digits=5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        #mean_tpr[0] = 0.0\n",
    "        #roc_auc = auc(fpr, tpr)\n",
    "        #ax1.plot(fpr, tpr, lw=1, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        #ax2.plot(recall, precision, lw=1, label='ROC fold %d (F1 = %0.2f)' % (i, f1))\n",
    "        #plt.plot(fpr, thresholds, lw=2, label='Decision Threshold at fold %d (%f)' % (i, np.mean(thresholds)))\n",
    "    \n",
    "#     ax1.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random Chance')\n",
    "#     ax1.set_xlim([-0.05, 1.05])\n",
    "#     ax1.set_ylim([-0.05, 1.05])\n",
    "#     ax1.set_xlabel('False Positive Rate\\n(% of Genuine Merchants Misclassified for Fraud)')\n",
    "#     ax1.set_ylabel('True Positive Rate\\n(% of All Fraudulent Merchants Caught from Dataset)')\n",
    "#     ax1.set_title('Receiver Operating Characteristic\\nLogistic Regression (Decision Threshold = {:02.1f})'.format(cutoff))\n",
    "#     ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    #ax2.set_xlim([-0.05, 1.05])\n",
    "    #ax2.set_ylim([-0.05, 1.05])\n",
    "#     ax2.set_xlabel('Sensitivity\\n(% of All Fraudulent Merchants Caught from Dataset)')\n",
    "#     ax2.set_ylabel('Precision\\n(% Fraudulent Merchants Correctly Identified)')\n",
    "#     ax2.set_title('Precision-Recall Curve\\nLogistic Regression (Decision Threshold = {:02.1f})'.format(cutoff))\n",
    "#     ax2.legend(loc=\"lower right\")\n",
    "    \n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ROC_PR(dat, ybin, 2, 500, 'balanced_subsample', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipelining classifiers\n",
    "estimators = [#('reduce_dim', PCA(n_components=0.90)), ('svm linear kernel', LinearSVC()), \n",
    "              ('logistic reg', LogisticRegression(n_jobs=-1))]\n",
    "              #('random forest', RandomForestClassifier(n_estimators=300, n_jobs=-1))]\n",
    "\n",
    "\n",
    "clf = Pipeline(estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(dat, ybin, stratify=ybin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macrof1 = f1_score(ytest, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(ytest, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest, preds, labels=label_bins.values(), \n",
    "                                    target_names=label_bins.keys(), \n",
    "                                    sample_weight=None, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
